{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Importing necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.a Enviroment creation\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "# 3.b Establishing parameters\n",
    "learning_rate = 0.1\n",
    "discount = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implementing SARSA\n",
    "\n",
    "# Auxiliar function to choose the next action\n",
    "def choose_next_action(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon: return env.action_space.sample()  # Exploration\n",
    "    else: return np.argmax(Q[state])  # Explotation\n",
    "\n",
    "# 4.a Implementing the SARSA algorithm\n",
    "def SARSA(env, episodes, learning_rate, discount, epsilon):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    Q = np.zeros((num_states, num_actions))  # Inicializing the action-value function Q\n",
    "\n",
    "    # Loop for each episode\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        state, _ = env.reset()                          # Initializing S\n",
    "        action = choose_next_action(Q, state, epsilon)  # Choosing A from S using policy derived from Q\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Taking action A, observing R, S'\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_action = choose_next_action(Q, next_state, epsilon)\n",
    "            Q[state, action] += learning_rate * (reward + (discount * Q[next_state, next_action]) - Q[state, action])\n",
    "                        \n",
    "            state = next_state      # S <- S'\n",
    "            action = next_action    # A <- A'\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # 4.b Returning the rewards per episode and the action-value function Q\n",
    "    return rewards_per_episode, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode, Q = SARSA(env, episodes, learning_rate, discount, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rewards_per_episode, columns=['reward'])\n",
    "df.to_csv('Results/SARSA_rewards.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
