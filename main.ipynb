{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Importing necessary libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3.a Enviroment creation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCliffWalking-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 3.b Establishing parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "# 3.a Enviroment creation\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "# 3.b Establishing parameters\n",
    "learning_rate = 0.1\n",
    "discount = 0.9\n",
    "epsilon = 0.1\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Implementing SARSA\n",
    "\n",
    "# Auxiliar function to choose the next action\n",
    "def choose_next_action(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon: return env.action_space.sample()  # Exploration\n",
    "    else: return np.argmax(Q[state])                                        # Explotation\n",
    "\n",
    "# 4.a Implementing the SARSA algorithm\n",
    "def SARSA(env, episodes, learning_rate, discount, epsilon):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    Q = np.zeros((num_states, num_actions))  # Inicializing the action-value function Q\n",
    "\n",
    "    # Loop for each episode\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        state, _ = env.reset()                          # Initializing S\n",
    "        action = choose_next_action(Q, state, epsilon)  # Choosing A from S using policy derived from Q\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Taking action A, observing R, S'\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            next_action = choose_next_action(Q, next_state, epsilon)\n",
    "            Q[state, action] += learning_rate * (reward + (discount * Q[next_state, next_action]) - Q[state, action])\n",
    "                        \n",
    "            state = next_state      # S <- S'\n",
    "            action = next_action    # A <- A'\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # 4.b Returning the rewards per episode and the action-value function Q\n",
    "    return rewards_per_episode, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode, Q = SARSA(env, episodes, learning_rate, discount, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rewards_per_episode, columns=['reward'])\n",
    "df.to_csv('Results/SARSA_rewards.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Q-Learning\n",
    "\n",
    "# Auxiliary function to choose the next action\n",
    "def choose_next_action(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        return env.action_space.sample()  # Exploration\n",
    "    else: \n",
    "        return np.argmax(Q[state])  # Exploitation\n",
    "\n",
    "# 4.a Implementing the Q-Learning algorithm\n",
    "def QLearning(env, episodes, learning_rate, discount, epsilon):\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = env.observation_space.n\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    Q = np.zeros((num_states, num_actions))  # Initializing the action-value function Q\n",
    "\n",
    "    # Loop for each episode\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        state, _ = env.reset()                          # Initializing S\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Loop for each step of episode\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = choose_next_action(Q, state, epsilon)  # Choose A from S using policy derived from Q\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Taking action A, observing R, S'\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Q-Learning update rule\n",
    "            Q[state, action] += learning_rate * (reward + (discount * np.max(Q[next_state])) - Q[state, action])\n",
    "                        \n",
    "            state = next_state  # S <- S'\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # 4.b Returning the rewards per episode and the action-value function Q\n",
    "    return rewards_per_episode, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards_per_episode, Q \u001b[38;5;241m=\u001b[39m QLearning(\u001b[43menv\u001b[49m, episodes, learning_rate, discount, epsilon)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "rewards_per_episode, Q = QLearning(env, episodes, learning_rate, discount, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
